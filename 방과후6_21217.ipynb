{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "방과후6_21217.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyODoBCZqYuvmW4DzhgYigLJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woo6244/1324/blob/master/%EB%B0%A9%EA%B3%BC%ED%9B%846_21217.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lanE_rXPIDxt",
        "colab_type": "text"
      },
      "source": [
        "# 웹 크롤링 -bs4, requests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOHSZNVzIIx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jmLTk0eITwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bs4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cunTpLjIIzyN",
        "colab_type": "text"
      },
      "source": [
        "## 01. 네이버 날씨:부산 현재 온도 가져오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQk8PTyOJIk6",
        "colab_type": "text"
      },
      "source": [
        "###웹 페이지 가져오기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06noAywbJOwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from pprint import pprint\n",
        "\n",
        "html =requests.get(\"https://weather.naver.com/rgn/cityWetrCity.nhn?cityRgnCd=CT008008\")\n",
        "pprint(html.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1shC31-N_An",
        "colab_type": "text"
      },
      "source": [
        "##파싱"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7EZb0lqOAL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup as bs \n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html =  requests.get(\"https://weather.naver.com/rgn/cityWetrCity.nhn?cityRgnCd=CT008008\")\n",
        "\n",
        "soup = bs(html.text, 'html.parser')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRYtbIbKP7h-",
        "colab_type": "text"
      },
      "source": [
        "###요소 찾기 find\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYmGyT0uP9CN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup as bs \n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html =  requests.get(\"https://weather.naver.com/rgn/cityWetrCity.nhn?cityRgnCd=CT008008\")\n",
        "\n",
        "soup = bs(html.text, 'html.parser')\n",
        "\n",
        "data1 = soup.find('div', {'class' : 'fl'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKNI_kEgP9nU",
        "colab_type": "text"
      },
      "source": [
        "###요서 모두 찾기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTZvxbheQD2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup as bs \n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html =  requests.get(\"https://weather.naver.com/rgn/cityWetrCity.nhn?cityRgnCd=CT008008\")\n",
        "\n",
        "soup = bs(html.text, 'html.parser')\n",
        "\n",
        "data1 = soup.find('div', {'class' : 'fl'})\n",
        "pprint(data1)\n",
        "\n",
        "data2 = data1.findAll('em')\n",
        "pprint(data2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPaEiUHcQn12",
        "colab_type": "text"
      },
      "source": [
        "###내부 텍스트 추출 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnSz4rI5QpeV",
        "colab_type": "code",
        "outputId": "57a560b6-9afd-4d94-cc78-c4259ef4e659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from bs4 import BeautifulSoup as bs \n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html =  requests.get(\"https://weather.naver.com/rgn/cityWetrCity.nhn?cityRgnCd=CT008008\")\n",
        "\n",
        "soup = bs(html.text, 'html.parser')\n",
        "\n",
        "data1 = soup.find('div', {'class' : 'fl'})\n",
        "##pprint(data1)\n",
        "\n",
        "data2 = data1.findAll('em')\n",
        "##pprint(data2)\n",
        "\n",
        "result = data2[2].text\n",
        "##pprint(result)\n",
        "\n",
        "present_temp = result.split()\n",
        "print(\"부산의 현재 날씨 정보 : \" , present_temp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "부산의 현재 날씨 정보 :  ['보통']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RBAveI1SoIn",
        "colab_type": "text"
      },
      "source": [
        "#네이버 웹툰 제목 가져오기\n",
        "\n",
        "```\n",
        "https://comic.naver.com/webtoon/weekday.nhn\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5Yi6-hCSqx9",
        "colab_type": "text"
      },
      "source": [
        "##월요 웹툰 영역"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk1S4IZmS6JC",
        "colab_type": "code",
        "outputId": "e9df2fa6-0784-41d8-89ac-d3191ce154d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "from bs4 import BeautifulSoup as bs4\n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html = requests.get(\"https://comic.naver.com/webtoon/weekday.nhn\")\n",
        "soup = bs4(html.text, 'html.parser')\n",
        "html.close()\n",
        "\n",
        "data1 = soup.find('div' , {'class':'cool_inner'})\n",
        "##pprint(soup)\n",
        "\n",
        "data2 = data1.findAll('a', {'class' : 'title'})\n",
        "##pprint(data2)\n",
        "\n",
        "title_list = []\n",
        "for i in data2 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-1bdb884eb1f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m##pprint(soup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Z2Mu1-V1HP",
        "colab_type": "text"
      },
      "source": [
        "###for 을 이용한 텍스트 추출\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0WKnBRTV3qt",
        "colab_type": "code",
        "outputId": "6b3a779d-5824-49a6-9089-e6316a65ad71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from bs4 import BeautifulSoup as bs4\n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html = requests.get(\"https://comic.naver.com/webtoon/weekday.nhn\")\n",
        "soup = bs4(html.text, 'html.parser')\n",
        "html.close()\n",
        "\n",
        "data1 = soup.find('div' , {'class':'cool_inner'})\n",
        "##pprint(soup)\n",
        "\n",
        "data2 = data1.findAll('a', {'class' : 'title'})\n",
        "##pprint(data2)\n",
        "\n",
        "title_list = []\n",
        "for i in data2 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-eacf45ec15cf>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    for i in data2\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe-oCmEHWVey",
        "colab_type": "text"
      },
      "source": [
        "###요일별 영역 가져오기from bs4 import BeautifulSoup as bs4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxAE7UU8WatV",
        "colab_type": "code",
        "outputId": "2a0b784d-e7af-4cfe-8862-048170cdd44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from bs4 import BeautifulSoup as bs4\n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html = requests.get(\"http://comic.naver.com/webtoon/weekday.nhn\")\n",
        "soup = bs4(html.text , 'html.parser')\n",
        "html.close()\n",
        "data_list = soup.findAll('div', {'class' : 'cool_inner'})\n",
        "pprint(data_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3ok2waDXulf",
        "colab_type": "text"
      },
      "source": [
        "####이하 코드 반복 시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCCfCnyaXwcN",
        "colab_type": "code",
        "outputId": "42e94f71-70d9-4a1e-ebdb-da1e468e9ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "from bs4 import BeautifulSoup as bs4\n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html = requests.get(\"http://comic.naver.com/webtoon/weekday.nhn\")\n",
        "soup = bs4(html.text , 'html.parser')\n",
        "html.close()\n",
        "data_list = soup.findAll('div', {'class' : 'cool_inner'})\n",
        "pprint(data_list)\n",
        "\n",
        "for data1 in data1_list:\n",
        "  data2 - data1.findAll('a', {'class' : 'title'})\n",
        "##pprint(data2)\n",
        "  title_list = [t.text for t in data2]\n",
        "  pprint(title_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-2e69b1aac873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata1_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mdata2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m##pprint(data2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data1_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIAdKwnhYo5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup as bs4\n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html = requests.get(\"http://comic.naver.com/webtoon/weekday.nhn\")\n",
        "soup = bs4(html.text , 'html.parser')\n",
        "html.close()\n",
        "data_list = soup.findAll('div', {'class' : 'cool_inner'})\n",
        "pprint(data_list)\n",
        "\n",
        "for data1 in data1_list:\n",
        "  data2 - data1.findAll('a', {'class' : 'title'})\n",
        "  ##pprint(data2)\n",
        "  title_list = [t.text for t in data2]\n",
        "  ##pprint(title_list)\n",
        "\n",
        "  week_title_list.extend(title_list)\n",
        "pprint(week_title_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eursIcRZR6e",
        "colab_type": "text"
      },
      "source": [
        "##네이버 웹툰 썸네일 가져오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKB_ed9AZSnN",
        "colab_type": "text"
      },
      "source": [
        "##제목과 썸네일이 같이 존재하는 영역"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVw6xnMxZqru",
        "colab_type": "code",
        "outputId": "50146dfc-b62e-42df-aa1b-092d65be5c9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from bs4 import BeautifulSoup as bs4\n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html = requests.get(\"https://comic.naver.com/webtoon/weekday.nhn\")\n",
        "soup = bs4(html.text, 'html.parser')\n",
        "html.close()\n",
        "\n",
        "data1_list = soup.findAll('div' , {'class' : 'cool_inner'})\n",
        "\n",
        "li_list = []\n",
        "for data1 in data1_list:\n",
        "  li_list.extend(data1.findAll('li'))\n",
        "\n",
        "pprint(li_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIDJmdZpa1zV",
        "colab_type": "text"
      },
      "source": [
        "### 제목과 이미지 링크 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyVYK6nta4re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup as bs4\n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html = requests.get(\"https://comic.naver.com/webtoon/weekday.nhn\")\n",
        "soup = bs4(html.text, 'html.parser')\n",
        "html.close()\n",
        "\n",
        "data1_list = soup.findAll('div' , {'class' : 'cool_inner'})\n",
        "\n",
        "li_list = []\n",
        "for data1 in data1_list:\n",
        "  li_list.extend(data1.findAll('li'))\n",
        "\n",
        "##pprint(li_list)\n",
        "for li in li_list:\n",
        "  lmg = li.find('img')\n",
        "  title = img['title']\n",
        "  img_src = img['src']\n",
        "  print(title,img_src)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VktOW0FbpEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup as bs4\n",
        "from pprint import pprint\n",
        "import requests\n",
        "\n",
        "html = requests.get(\"https://comic.naver.com/webtoon/weekday.nhn\")\n",
        "soup = bs4(html.text, 'html.parser')\n",
        "html.close()\n",
        "\n",
        "data1_list = soup.findAll('div' , {'class' : 'cool_inner'})\n",
        "\n",
        "li_list = []\n",
        "for data1 in data1_list:\n",
        "  li_list.extend(data1.findAll('li'))\n",
        "\n",
        "##pprint(li_list)\n",
        "for li in li_list:\n",
        "  lmg = li.find('img')\n",
        "  title = img['title']\n",
        "  img_src = img['src']\n",
        "  ##print(title,img_src)\n",
        "  urlretrleve(img_src, title+'.jpg')\n",
        "  urlretrleve(img, /content/drive/My Drive)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}